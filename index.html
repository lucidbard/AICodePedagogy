<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Digging into AI: An Archaelogical Python Adventure</title>

  <!-- Google Fonts for Colab-style appearance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&family=Roboto+Mono:wght@400;500&display=swap"
    rel="stylesheet">

  <script>
    // Fallback function to load Skulpt from alternative CDN
    function loadSkulptFallback() {
      console.log('Trying fallback CDN for Skulpt...');
      const script1 = document.createElement('script');
      script1.src = 'https://cdnjs.cloudflare.com/ajax/libs/skulpt/0.11.1/skulpt.min.js';
      script1.onload = () => {
        console.log('Skulpt loaded from fallback');
        if (window.onSkulptLoaded) window.onSkulptLoaded();
      };
      script1.onerror = () => console.error('All CDNs failed for Skulpt');
      document.head.appendChild(script1);

      const script2 = document.createElement('script');
      script2.src = 'https://cdnjs.cloudflare.com/ajax/libs/skulpt/0.11.1/skulpt-stdlib.js';
      script2.onload = () => console.log('Skulpt stdlib loaded from fallback');
      document.head.appendChild(script2);
    }
  </script>

  <script src="https://unpkg.com/skulpt@0.11.1/dist/skulpt.min.js"
    onload="if(window.onSkulptLoaded) window.onSkulptLoaded(); console.log('Skulpt core loaded')"
    onerror="loadSkulptFallback()"></script>
  <script src="https://unpkg.com/skulpt@0.11.1/dist/skulpt-stdlib.js" onload="console.log('Skulpt stdlib loaded')"
    onerror="console.error('Failed to load Skulpt stdlib')"></script>

  <!-- CodeMirror for syntax highlighting -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/codemirror.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/theme/material-darker.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/mode/python/python.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/addon/edit/closebrackets.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/addon/edit/matchbrackets.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/addon/hint/show-hint.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/addon/hint/python-hint.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.65.16/addon/hint/show-hint.min.css">

  <link rel="stylesheet" href="style.css" />
</head>

<body>
  <div class="container">
    <div class="header">
      <h1 id="game-title">Digging into AI</h1>
      <p class="subtitle" id="game-subtitle">An Archaelogical Python Adventure</p>
      <div class="progress-bar">
        <div class="progress-fill" id="progress-bar"></div>
      </div>
    </div>

    <div class="game-area">
      <div class="story-panel">
        <h2>The Mystery</h2>
        <div class="story-text" id="story-content">
          <!-- Story content will be loaded here -->
        </div>
        <div class="challenge-text" id="challenge-content">
          <!-- Challenge description will be loaded here -->
        </div>
        <div class="data-text" id="data-content">
          <!-- Data description will be loaded here -->
        </div>
      </div>
      <div class="code-panel">
        <h2>Your Code</h2>
        <div id="cells-container">
          <!-- For multi-cell stages -->
        </div>
        <div id="single-cell-container">
          <!-- For single-cell stages - will be dynamically populated -->
        </div>
        <div class="hint-section">
          <div id="hints-container">
            <!-- Hint buttons will be generated here -->
          </div>
          <div id="hint-text-container">
            <!-- Hint text will appear here -->
          </div>
        </div>
        <div class="runtime-controls">
          <button class="restart-runtime-button" id="restart-runtime-button"
            title="Clear all cell outputs and reset runtime (similar to restarting a Jupyter kernel)">üîÑ Restart
            Runtime</button>
          <button class="clear-progress-button" id="clear-progress-button"
            title="Clear all saved progress and start from the beginning">üóëÔ∏è Clear Progress</button>
        </div>
        <button class="next-button" id="next-button">Continue to Next Stage ‚Üí</button>
      </div>
    </div>
  </div>

  <!-- Developer navigation for testing -->
  <button class="dev-nav-toggle" id="dev-nav-toggle">‚öô</button>
  <div class="dev-nav" id="dev-nav">
    <!-- Stage navigation buttons will be generated here -->
  </div>
  <!-- Celebration effect container -->
  <div class="celebration" id="celebration"></div>
  <!-- LLM Assistant Footer -->
  <footer class="llm-footer">
    <div class="llm-footer-content">
      <div class="llm-toggle-container">
        <label class="llm-toggle">
          <input type="checkbox" id="llm-enabled" />
          <span class="llm-slider"></span>
          <span class="llm-label">ü§ñ AI Assistant
          </span>
        </label>
      </div>
      <button id="ollama-help-btn" class="help-btn" title="How to setup Ollama">‚ùì</button>
      <div class="llm-settings" id="llm-settings" style="display: none;">
        <div class="provider-selection">
          <label for="provider-select">Provider:</label>
          <select id="provider-select">
            <option value="ollama">Ollama (Local)</option>
            <option value="openai">OpenAI</option>
            <option value="anthropic">Anthropic</option>
          </select>
        </div>
        <div class="api-key-container" id="api-key-container" style="display: none;">
          <input type="password" id="api-key-input" placeholder="Enter API key">
          <button id="save-api-key" class="save-key-btn">Save</button>
        </div>
        <span class="llm-model-info" id="llm-model-info">No model selected</span>
        <button id="change-model" class="change-model-btn" title="Change AI model">Change Model</button>
        <div class="model-selection" id="model-selection" style="display: none;">
          <select id="model-select">
            <option value="">Select a model...</option>
          </select>
          <button id="refresh-models" class="refresh-btn" title="Refresh available models">üîÑ</button>
        </div>
      </div>
      <div class="llm-status" id="llm-status"></div>
    </div>
  </footer>

  <!-- Ollama Help Modal -->
  <div class="modal-overlay" id="ollama-help-modal" style="display: none;">
    <div class="modal-content">
      <div class="modal-header">
        <h3>ü§ñ Setting up Ollama for AI Assistant</h3>
        <button class="modal-close" id="close-ollama-help">&times;</button>
      </div>
      <div class="modal-body">
        <div class="help-section">
          <h4>üì¶ Step 1: Install Ollama</h4>
          <p>Download and install Ollama from <a href="https://ollama.ai" target="_blank">https://ollama.ai</a></p>

          <div class="platform-tabs">
            <button class="tab-btn-install active" data-platform="windows">Windows</button>
            <button class="tab-btn-install" data-platform="mac">macOS/Linux</button>
          </div>

          <div class="platform-content-install" data-platform="windows">
            <div class="code-block">
              <strong>Windows:</strong><br>
              Download the installer from the website
            </div>
          </div>

          <div class="platform-content-install" data-platform="mac" style="display: none;">
            <div class="code-block">
              <strong>macOS/Linux:</strong><br>
              <code>curl -fsSL https://ollama.ai/install.sh | sh</code>
            </div>
          </div>
        </div>

        <div class="help-section" id="cors-section">
          <h4>üîß Step 2: Configure CORS for Web Access</h4>
          <p id="cors-description">To allow this website (<strong>https://jtm.io/codepedagogy/</strong>) to connect to
            your local Ollama
            server, you need to set environment variables:</p>

          <div class="platform-tabs">
            <button class="tab-btn active" data-platform="windows">Windows</button>
            <button class="tab-btn" data-platform="mac">macOS/Linux</button>
          </div>

          <div class="platform-content" data-platform="windows">
            <h5>Windows Setup:</h5>
            <div class="code-block">
              <strong>Option 1 - Command Prompt:</strong><br>
              <code>set OLLAMA_ORIGINS=https://jtm.io/codepedagogy/,http://localhost:*</code><br>
              <code>ollama serve</code>
            </div>
            <div class="code-block">
              <strong>Option 2 - PowerShell:</strong><br>
              <code>$env:OLLAMA_ORIGINS="https://jtm.io/codepedagogy/,http://localhost:*"</code><br>
              <code>ollama serve</code>
            </div>
            <div class="code-block">
              <strong>Option 3 - Permanent (System Environment Variables):</strong><br>
              1. Press <code>Win + R</code>, type <code>sysdm.cpl</code><br>
              2. Click "Environment Variables"<br>
              3. Add new system variable:<br>
              &nbsp;&nbsp;Name: <code>OLLAMA_ORIGINS</code><br>
              &nbsp;&nbsp;Value: <code>https://jtm.io/codepedagogy/,http://localhost:*</code><br>
              4. Restart command prompt and run <code>ollama serve</code>
            </div>
          </div>

          <div class="platform-content" data-platform="mac" style="display: none;">
            <h5>macOS/Linux Setup:</h5>
            <div class="code-block">
              <strong>Terminal (temporary):</strong><br>
              <code>export OLLAMA_ORIGINS="https://jtm.io/codepedagogy/,http://localhost:*"</code><br>
              <code>ollama serve</code>
            </div>
            <div class="code-block">
              <strong>Permanent (add to ~/.bashrc, ~/.zshrc, or ~/.profile):</strong><br>
              <code>echo 'export OLLAMA_ORIGINS="https://jtm.io/codepedagogy/,http://localhost:*"' >> ~/.bashrc</code><br>
              <code>source ~/.bashrc</code><br>
              <code>ollama serve</code>
            </div>
            <div class="code-block">
              <strong>Using launchctl (macOS service):</strong><br>
              <code>launchctl setenv OLLAMA_ORIGINS "https://jtm.io/codepedagogy/,http://localhost:*"</code><br>
              <code>ollama serve</code>
            </div>
          </div>
        </div>

        <div class="help-section" id="install-model-section">
          <h4 id="install-model-heading">üöÄ Step 3: Install a Model</h4>
          <p>Download a model suitable for coding assistance:</p>
          <div class="code-block">
            <strong>Recommended models:</strong><br>
            <code>ollama pull codellama:7b</code> (good for coding, 4GB)<br>
            <code>ollama pull llama3.1:8b</code> (general purpose, 5GB)<br>
            <code>ollama pull qwen2.5-coder:7b</code> (excellent for coding, 4GB)
          </div>
        </div>

        <div class="help-section" id="test-connection-section">
          <h4 id="test-connection-heading">‚úÖ Step 4: Test Connection</h4>
          <p id="test-connection-description">Once Ollama is running with CORS configured:</p>
          <ol>
            <li>Enable the "ü§ñ AI Assistant" toggle above</li>
            <li>Select your model from the dropdown</li>
            <li>Look for "Connected" status</li>
          </ol>
        </div>

        <div class="help-section warning">
          <h4>‚ö†Ô∏è Important Security Notes</h4>
          <ul>
            <li>This configuration allows the specific website to access your local Ollama instance</li>
            <li>Only use this setup on trusted networks</li>
            <li>Stop Ollama when not needed: <code>Ctrl+C</code> in the terminal</li>
            <li>Your models and conversations stay completely private on your machine</li>
          </ul>
        </div>

        <div class="help-section">
          <h4>üîç Troubleshooting</h4>
          <ul>
            <li><strong>Connection failed:</strong> Ensure Ollama is running and CORS is configured</li>
            <li><strong>Model not found:</strong> Make sure you've pulled the model with
              <code>ollama pull model-name</code>
            </li>
            <li><strong>Slow responses:</strong> Try a smaller model or check your system resources</li>
            <li><strong>Windows issues:</strong> Try running Command Prompt as Administrator</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <script src="llm-integration.js"></script>
  <script src="script.js"></script>
</body>

</html>